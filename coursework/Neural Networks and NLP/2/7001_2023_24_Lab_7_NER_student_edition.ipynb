{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Lab 7 2023/24: NER**"
      ],
      "metadata": {
        "id": "nFqAT2TH5EYA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports"
      ],
      "metadata": {
        "id": "Mx1GOqs55iBH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Lxy43VPR4Kr9"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import Input,Model\n",
        "from tensorflow.keras.layers import Dropout,Dense,GRU,Bidirectional\n",
        "import numpy as np\n",
        "import json,time,collections,random"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Class NERModel"
      ],
      "metadata": {
        "id": "843qGjmR5vbP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NERModel(object):\n",
        "  def __init__(self,embedding_path, embedding_size,ner_labels):\n",
        "    self.embedding_path = embedding_path\n",
        "    self.embedding_size = embedding_size\n",
        "    self.embedding_dropout_rate = 0.5\n",
        "    self.hidden_size = 50\n",
        "    self.ffnn_layer = 2\n",
        "    self.hidden_dropout_rate = 0.2\n",
        "    self.embedding_dict = self.load_embeddings()\n",
        "    self.ner_labels = ner_labels\n",
        "    self.ner_labels_mappings = {l:i for i,l in enumerate(ner_labels)}\n",
        "\n",
        "  def load_embeddings(self):\n",
        "    print(\"Loading word embeddings from {}...\".format(self.embedding_path))\n",
        "    embeddings = collections.defaultdict(lambda: np.zeros(self.embedding_size))\n",
        "    for line in open(self.embedding_path):\n",
        "      splitter = line.find(' ')\n",
        "      emb = np.fromstring(line[splitter + 1:], np.float32, sep=' ')\n",
        "      assert len(emb) == self.embedding_size\n",
        "      embeddings[line[:splitter]] = emb\n",
        "    print(\"Finished loading word embeddings\")\n",
        "    return embeddings\n",
        "\n",
        "  def build(self):\n",
        "    word_embeddings = Input(shape=(None,self.embedding_size,))\n",
        "    word_embeddings = Dropout(self.embedding_dropout_rate)(word_embeddings)\n",
        "    \"\"\"\n",
        "    Task 1 Create a two layer Bidirectional GRU and Multi-layer FFNN to compute the ner scores for individual tokens\n",
        "    The shape of the ner_scores is [batch_size, max_sentence_length, number_of_ner_labels]\n",
        "    \"\"\"\n",
        "\n",
        "    # Create a two-layer Bidirectional GRU\n",
        "    # First layer\n",
        "    word_output = Bidirectional(GRU(self.hidden_size,\n",
        "                                    return_sequences=True,\n",
        "                                    recurrent_dropout=self.hidden_dropout_rate))(word_embeddings)\n",
        "    # Second layer\n",
        "    word_output = Bidirectional(GRU(self.hidden_size,\n",
        "                                    return_sequences=True,\n",
        "                                    recurrent_dropout=self.hidden_dropout_rate))(word_output)\n",
        "\n",
        "    # Apply dropout to the output of the GRU\n",
        "    word_output = Dropout(self.hidden_dropout_rate)(word_output)\n",
        "\n",
        "    # Create a multi-layer FFNN with two hidden layers\n",
        "    # First hidden layer\n",
        "    ffnn_output = Dense(self.hidden_size, activation='relu')(word_output)\n",
        "    # Apply dropout to the first hidden layer\n",
        "    ffnn_output = Dropout(self.hidden_dropout_rate)(ffnn_output)\n",
        "    # Second hidden layer\n",
        "    ffnn_output = Dense(self.hidden_size, activation='relu')(ffnn_output)\n",
        "    # Apply dropout to the second hidden layer\n",
        "    ffnn_output = Dropout(self.hidden_dropout_rate)(ffnn_output)\n",
        "\n",
        "    # Output layer to compute the ner_scores\n",
        "    ner_scores = Dense(len(self.ner_labels_mappings), activation='softmax')(ffnn_output)\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    End Task 1\n",
        "    \"\"\"\n",
        "\n",
        "    self.model = Model(inputs=[word_embeddings],outputs=ner_scores)\n",
        "    self.model.compile(optimizer='adam',loss=\"sparse_categorical_crossentropy\",metrics=['accuracy'])\n",
        "    self.model.summary()\n",
        "\n",
        "\n",
        "\n",
        "  def get_feed_dict_list(self, path,batch_size):\n",
        "    feed_dict_list = []\n",
        "    data_sets = json.loads(open(path).readlines()[0])\n",
        "    sentences = data_sets['sentences']\n",
        "    ners = data_sets['ners']\n",
        "    for i in range(0,len(sentences),batch_size):\n",
        "      batch_start, batch_end = i, min(i+batch_size, len(sentences))\n",
        "      sent_lengths = [len(sent) for sent in sentences[batch_start:batch_end]]\n",
        "      max_sent_length = max(sent_lengths)\n",
        "\n",
        "      word_emb = np.zeros([len(sent_lengths), max_sent_length, self.embedding_size])\n",
        "      for i, sent in enumerate(sentences[batch_start:batch_end]):\n",
        "        for j, word in enumerate(sent):\n",
        "          word_emb[i, j] = self.embedding_dict[word.lower()]\n",
        "\n",
        "      word_ner_labels = np.zeros([len(sent_lengths), max_sent_length])\n",
        "      gold_named_entities = set()\n",
        "      for i, ner in enumerate(ners[batch_start:batch_end]):\n",
        "        for s,e,l in ner:\n",
        "          l_id = self.ner_labels_mappings[l]\n",
        "          gold_named_entities.add((i,s,e,l_id))\n",
        "          for j in range(s,e+1):\n",
        "            word_ner_labels[i,j] = l_id\n",
        "\n",
        "\n",
        "      feed_dict_list.append((\n",
        "        word_emb,\n",
        "        word_ner_labels,\n",
        "        gold_named_entities,\n",
        "        sent_lengths\n",
        "      ))\n",
        "\n",
        "    return feed_dict_list\n",
        "\n",
        "\n",
        "  def batch_generator(self, fd_list):\n",
        "    random.shuffle(fd_list)\n",
        "    for word_embeddings, word_ner_labels, _, _ in fd_list:\n",
        "      yield [word_embeddings], word_ner_labels\n",
        "\n",
        "  def train(self, train_path, dev_path, test_path, epochs,batch_size=100):\n",
        "    train_fd_list = self.get_feed_dict_list(train_path,batch_size)\n",
        "    print(\"Load {} training batches from {}\".format(len(train_fd_list), train_path))\n",
        "\n",
        "    dev_fd_list = self.get_feed_dict_list(dev_path,batch_size)\n",
        "    print(\"Load {} dev batches from {}\".format(len(dev_fd_list), dev_path))\n",
        "\n",
        "    test_fd_list = self.get_feed_dict_list(test_path,batch_size)\n",
        "    print(\"Load {} test batches from {}\".format(len(test_fd_list), test_path))\n",
        "\n",
        "    start_time = time.time()\n",
        "    for epoch in range(epochs):\n",
        "      print(\"\\nStarting training epoch {}/{}\".format(epoch + 1, epochs))\n",
        "      epoch_time = time.time()\n",
        "\n",
        "      self.model.fit(self.batch_generator(train_fd_list), steps_per_epoch=len(train_fd_list))\n",
        "\n",
        "      print(\"Time used for epoch {}: {}\".format(epoch + 1, self.time_used(epoch_time)))\n",
        "      dev_time = time.time()\n",
        "      print(\"Evaluating on dev set after epoch {}/{}:\".format(epoch + 1, epochs))\n",
        "      self.eval(dev_fd_list)\n",
        "      print(\"Time used for evaluate on dev set: {}\".format(self.time_used(dev_time)))\n",
        "\n",
        "    print(\"\\nTraining finished!\")\n",
        "    print(\"Time used for training: {}\".format(self.time_used(start_time)))\n",
        "\n",
        "    print(\"\\nEvaluating on test set:\")\n",
        "    test_time = time.time()\n",
        "    self.eval(test_fd_list)\n",
        "    print(\"Time used for evaluate on test set: {}\".format(self.time_used(test_time)))\n",
        "\n",
        "  def eval(self, eval_fd_list):\n",
        "    tp, fn, fp = 0,0,0\n",
        "    for word_embeddings, _, gold,sent_lens in eval_fd_list:\n",
        "      predictions = self.model.predict_on_batch([word_embeddings])\n",
        "\n",
        "      \"\"\"\n",
        "      Task 2 create the predictions of NER from the IO label\n",
        "      e.g.\n",
        "      0 I         O\n",
        "      1 met       O\n",
        "      2 John      PER\n",
        "      3 this      O\n",
        "      4 afternoon O\n",
        "      should give you a person NE John (x,2,2,1)\n",
        "      where x is the sentence id in the batch, and 2,2 are the start and end indices of the NE,\n",
        "      1 is the id for 'PER'\n",
        "      \"\"\"\n",
        "      predicted_labels = np.argmax(predictions, axis=-1)\n",
        "      for i, sent_len in enumerate(sent_lens):\n",
        "            pred_entities = set()\n",
        "            gold_i = {(idx, s, e, l) for idx, s, e, l in gold if idx == i}\n",
        "\n",
        "            start = None\n",
        "            current_label = None\n",
        "            for j in range(sent_len):\n",
        "                label = predicted_labels[i, j]\n",
        "                if label != self.ner_labels_mappings['O']:\n",
        "                    if start is None:\n",
        "                        start = j\n",
        "                        current_label = label\n",
        "                    elif label != current_label:\n",
        "                        pred_entities.add((i, start, j-1, current_label))\n",
        "                        start = j\n",
        "                        current_label = label\n",
        "                else:\n",
        "                    if start is not None:\n",
        "                        pred_entities.add((i, start, j-1, current_label))\n",
        "                        start = None\n",
        "                        current_label = None\n",
        "            # Check for an entity ending at the last token\n",
        "            if start is not None:\n",
        "                pred_entities.add((i, start, sent_len-1, current_label))\n",
        "\n",
        "\n",
        "            tp += len(pred_entities & gold_i)\n",
        "            fp += len(pred_entities - gold_i)\n",
        "            fn += len(gold_i - pred_entities)\n",
        "\n",
        "\n",
        "      \"\"\"\n",
        "      End Task 2\n",
        "      \"\"\"\n",
        "\n",
        "    p = 0.0 if tp == 0 else tp*1.0/(tp+fp)\n",
        "    r = 0.0 if tp == 0 else tp*1.0/(tp+fn)\n",
        "    f = 0.0 if tp == 0 else 2*p*r/(p+r)\n",
        "    print(\"F1 : {:.2f}%\".format(f * 100))\n",
        "    print(\"Precision: {:.2f}%\".format(p * 100))\n",
        "    print(\"Recall: {:.2f}%\".format(r * 100))\n",
        "\n",
        "  def time_used(self, start_time):\n",
        "    curr_time = time.time()\n",
        "    used_time = curr_time - start_time\n",
        "    m = used_time // 60\n",
        "    s = used_time - 60 * m\n",
        "    return \"%d m %d s\" % (m, s)"
      ],
      "metadata": {
        "id": "81urrGnf5xan"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Run**"
      ],
      "metadata": {
        "id": "m6c1Q6eL6MBe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "  embedding_path = 'glove.6B.100d.txt.ner.filtered'\n",
        "  train_path = 'train.conll03.json'\n",
        "  dev_path = 'dev.conll03.json'\n",
        "  test_path = 'test.conll03.json'\n",
        "  ner_labels = ['O', 'PER', 'ORG', 'LOC', 'MISC']\n",
        "  embedding_size = 100\n",
        "  model = NERModel(embedding_path,embedding_size, ner_labels)\n",
        "  model.build()\n",
        "  model.train(train_path,dev_path,test_path,5)"
      ],
      "metadata": {
        "id": "bE5jNCvS6OH0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c605e416-68d0-4c57-fe66-efd53c87beaa"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading word embeddings from glove.6B.100d.txt.ner.filtered...\n",
            "Finished loading word embeddings\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, None, 100)]       0         \n",
            "                                                                 \n",
            " bidirectional (Bidirection  (None, None, 100)         45600     \n",
            " al)                                                             \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirecti  (None, None, 100)         45600     \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, None, 100)         0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, None, 50)          5050      \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, None, 50)          0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, None, 50)          2550      \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, None, 50)          0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, None, 5)           255       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 99055 (386.93 KB)\n",
            "Trainable params: 99055 (386.93 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Load 141 training batches from train.conll03.json\n",
            "Load 33 dev batches from dev.conll03.json\n",
            "Load 35 test batches from test.conll03.json\n",
            "\n",
            "Starting training epoch 1/5\n",
            "141/141 [==============================] - 55s 308ms/step - loss: 0.3558 - accuracy: 0.9400\n",
            "Time used for epoch 1: 1 m 33 s\n",
            "Evaluating on dev set after epoch 1/5:\n",
            "F1 : 31.46%\n",
            "Precision: 48.88%\n",
            "Recall: 23.19%\n",
            "Time used for evaluate on dev set: 0 m 6 s\n",
            "\n",
            "Starting training epoch 2/5\n",
            "141/141 [==============================] - 41s 290ms/step - loss: 0.1025 - accuracy: 0.9683\n",
            "Time used for epoch 2: 0 m 41 s\n",
            "Evaluating on dev set after epoch 2/5:\n",
            "F1 : 64.74%\n",
            "Precision: 73.13%\n",
            "Recall: 58.08%\n",
            "Time used for evaluate on dev set: 0 m 2 s\n",
            "\n",
            "Starting training epoch 3/5\n",
            "141/141 [==============================] - 41s 288ms/step - loss: 0.0697 - accuracy: 0.9793\n",
            "Time used for epoch 3: 0 m 40 s\n",
            "Evaluating on dev set after epoch 3/5:\n",
            "F1 : 74.37%\n",
            "Precision: 75.84%\n",
            "Recall: 72.96%\n",
            "Time used for evaluate on dev set: 0 m 3 s\n",
            "\n",
            "Starting training epoch 4/5\n",
            "141/141 [==============================] - 42s 299ms/step - loss: 0.0567 - accuracy: 0.9829\n",
            "Time used for epoch 4: 0 m 42 s\n",
            "Evaluating on dev set after epoch 4/5:\n",
            "F1 : 78.11%\n",
            "Precision: 80.52%\n",
            "Recall: 75.83%\n",
            "Time used for evaluate on dev set: 0 m 2 s\n",
            "\n",
            "Starting training epoch 5/5\n",
            "141/141 [==============================] - 41s 289ms/step - loss: 0.0491 - accuracy: 0.9852\n",
            "Time used for epoch 5: 0 m 40 s\n",
            "Evaluating on dev set after epoch 5/5:\n",
            "F1 : 80.47%\n",
            "Precision: 81.14%\n",
            "Recall: 79.80%\n",
            "Time used for evaluate on dev set: 0 m 2 s\n",
            "\n",
            "Training finished!\n",
            "Time used for training: 4 m 34 s\n",
            "\n",
            "Evaluating on test set:\n",
            "F1 : 76.81%\n",
            "Precision: 77.05%\n",
            "Recall: 76.58%\n",
            "Time used for evaluate on test set: 0 m 2 s\n"
          ]
        }
      ]
    }
  ]
}